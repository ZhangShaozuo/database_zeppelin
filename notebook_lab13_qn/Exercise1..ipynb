{
  "metadata": {
    "name": "Exercise1",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Prelude: A crash course of Zeppelin Notebook\n\nSimilar to Jupyter Notebook, there are at least two kinds of Cells in a Zeppelin Notebook.\n\n1. Markdown Cell - start with `%md`\n2. Code Cell - starting with `%pyspark`\n\nTo run a cell, press the \"play\" button on its right, or press \"shift-enter\"\n\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n\nprint(\"This is a code block\")\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " \n\n# Exercise 1\n\nIn this exercise we are tasked to perform some data transformation using PySpark and RDD.\n\nFor parts marked with **[CODE CHANGE REQUIRED]** you need to modify or complete the code before execution.\nFor parts without **[CODE CHANGE REQUIRED]** , you can just run the given code."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Input\n\nThe input data are stored in a text file `data/ex1/input.txt` which is a list of 2D coordinates in the following format.\n\n```text\n\u003clabel\u003e 0:\u003cx-value\u003e 1:\u003cy-value\u003e \n...\n\u003clabel\u003e 0:\u003cx-value\u003e 1:\u003cy-value\u003e\n```\n\n## Output\n\nThe expected output of the transformation are two seperate TSV outputs, `ones` and `zeros`. Both are in the following format\n\n```text\n\u003cx-value\u003e \u003cy-value\u003e ...\n\u003cx-value\u003e \u003cy-value\u003e\n```\n\n\nFor example, given the input file as the following\n\n```text\n1 0:102 1:230\n0 0:123 1:56\n0 0:22  1:2\n1 0:74 1:102\n```\nThe output files in `ones` would be \n\n```text\n102 230\n74 102\n```\n\nThe output files in `zeros` would be\n\n```text\n123 56 \n22 2\n```\nwhere the space in between the numbers are tabs"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "**[CODE CHANGE REQUIRED]** \nModify and run the following bash cell to upload the input data to HDFS"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%sh\n# export PATH\u003d$PATH:/opt/hadoop-3.3.0/bin/\n\n# hdfs dfs -rm -r /lab13/ex1\nhdfs dfs -mkdir -p /lab13/ex1/input/\nhdfs dfs -put /home/hadoop/git/sutd50043_student/lab13/data/ex1/input.txt  /lab13/ex1/input/\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "**[CODE CHANGE REQUIRED]** \n\nComplete the following PySpark script so that it performs the above-mentioned transformation.\n\n\u003cstyle\u003e\n    div.hidecode + pre {display: none}\n\u003c/style\u003e\n\u003cscript\u003e\ndoclick\u003dfunction(e) {\n    e.nextSibling.nextSibling.style.display\u003d\"block\";\n}\n\u003c/script\u003e\n\n\u003cdiv class\u003d\"hidecode\" onclick\u003d\"doclick(this);\"\u003e[Show Hint]\u003c/div\u003e\n\n```text\nLet r be an RDD, r.map(f) applies f to all elements in r and return a new RDD.\nr.filter(p) fitlers out elements e from r that satisfying p(e).\nr.saveAsTextFile(\"hdfs://...\") will save an RDD into hdfs.\n```"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n\nfrom pyspark.sql import SparkSession\nsparkSession \u003d SparkSession.builder.appName(\"Transformation notebook\").getOrCreate()\nsc \u003d sparkSession.sparkContext\n\nhdfs_nn \u003d \"172.31.67.32\"\ndef join(tokenized):\n    x \u003d (tokenized[1].split(\":\"))[1] \n    y \u003d (tokenized[2].split(\":\"))[1]\n    return \"\\t\".join([x,y])\n\nsc.appName \u003d \"ETL (Transform) Example\"\n\ninput \u003d sc.textFile(\"hdfs://%s:9000/lab13/ex1/input/\" % hdfs_nn) \n\ntokenizeds \u003d input.map(lambda line : line.split(\" \")) \ntokenizeds.persist()\n\n# TODO: fix me\nones \u003d None\nones.saveAsTextFile(\"hdfs://%s:9000/lab13/ex1/output/ones\" % hdfs_nn)\n\n# TODO: fix me\nzeros \u003d None\nzeros.saveAsTextFile(\"hdfs://%s:9000/lab13/ex1/output/zeros\" %hdfs_nn) \n\nsc.stop()\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Test case\n\nRun the fulling bash cell to check the results\n\nIt should be something like the following \n\n```text\n20/11/12 18:51:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n124\t253\n145\t255\n5\t63\n1\t168\n121\t254\n166\t222\n178\t255\n7\t176\n68\t45\n...\n```\n\nand \n\n```text\n124\t253\n145\t255\n5\t63\n1\t168\n121\t254\n166\t222\n178\t255\n7\t176\n68\t45\n64\t191\n...\n```\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%sh\nexport PATH\u003d$PATH:/opt/hadoop-3.3.0/bin/\n\nhdfs dfs -cat /lab13/ex1/output/ones/* \n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%sh\nexport PATH\u003d$PATH:/opt/hadoop-3.3.0/bin/\nhdfs dfs -cat /lab13/ex1/output/zeros/* "
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Cleaning up\n\nModify the following bash cell to clean HDFS"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%sh\nexport PATH\u003d$PATH:/opt/hadoop-3.3.0/bin/\n\nhdfs dfs -rm -r /lab13/ex1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " \n\n## End of Exercise 1\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ""
    }
  ]
}