{
  "metadata": {
    "name": "Exercise3",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Exercise 3\n\nIn this exercise, we are going to implement the KMeans clustering algorithm using Spark RDD.\n\nFor parts marked with **[CODE CHANGE REQUIRED]** you need to modify or complete the code before execution.\nFor parts without **[CODE CHANGE REQUIRED]** , you can just run the given code.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## KMeans clustering algorithm\n\nKMeans clustering algorithm is an unsupervised machine learning algorithm which groups data points into *cluster*s (or groups) based on their similarity, e.g. product their likes, movies their likes, brands they follow, TV programs and movies they watched, university and colleages they attended. \n\nAssume all the attributes of the subjects in the analysis can be represented using some scalar values, we can conduct the analysis in the following steps.\n\n1. The user/programmer specifies how many clusters he/she would like to group all the data points under. Let\u0027s say it is `K`\n2. Randomly generate `K` data points, we call them *centroids*, `c1, c2, ..., cK`.\n3. For each data point `p`, we compute the distances between `p` and `c1`, `p` and `c2`, ... Find the centroid `ci`, to which `p` is closest, we conclude `p` is in cluster `i`.\n4. For each cluster `i`, we retrieve all the data points falling in this cluster, and compute the mean. The mean will be new centroid for cluster `i`, say `ci\u0027`\n5. Compare `c1` with `c1\u0027`, `c2` with `c2\u0027`, ..., `cK` with `cK\u0027`. If all of them remains unchanged (or the differnce is lower than a threshold), we are done. Otherwise, update `c1 \u003d c1\u0027`, ..., `cK \u003d cK` and go back to step 3.\n\n\nPoint to note, clusters might be disappear, i.e. some centroid has zero data point inside. \n\n![](https://i.stack.imgur.com/ibYKU.png)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Instead of using real data, we use a python script to generate the data. The python script is already written for you. It can be found in the git-cloned repo `sutd50043_student/lab13/data/ex3/data/generate.py` \n\n```python\nimport sys\nimport random\ndef gen(num_of_records, filename):\n    with open(filename,\u0027w\u0027) as f:\n        for i in range(0,int(num_of_records)):\n            x \u003d random.uniform(-100,100)\n            y \u003d random.uniform(-100,100) \n            f.write(\"%.2f\\t%.2f\\n\" % (x,y))\n    f.close()\n\nif __name__ \u003d\u003d \"__main__\":\n    if len(sys.argv) \u003e 2:\n        sys.exit(gen(sys.argv[1],sys.argv[2]))\n    else:\n        print(\"USAGE: python3 generate.py \u003cnumber_of_records\u003e \u003cfile_name\u003e\")\n```"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " **[CODE CHANGE REQUIRED]**\nModify the bash command in the next cell accordingly to execute the above script.\n\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%sh\n\npython3 /home/hadoop/git/sutd50043_student/lab13/data/ex3/generate.py 10000000 points.tsv\npython3 /home/hadoop/git/sutd50043_student/lab13/data/ex3/generate.py 10 centroids.tsv\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## TODO 0\n\n **[CODE CHANGE REQUIRED]**\n \nModify the following bash command and execute it, so that we can upload the points in the HDFS"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%sh\n\nexport PATH\u003d$PATH:/opt/hadoop-3.3.0/bin/\nhdfs dfs -mkdir -p /lab13/ex3/input/\nhdfs dfs -put points.tsv /lab13/ex3/input/\n\nhdfs dfs -mkdir -p /lab13/ex3/centroids/\nhdfs dfs -put centroids.tsv /lab13/ex3/centroids/\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " \n\nWe load the spark session to create the spark context"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\nfrom pyspark.sql import SparkSession\nsparkSession \u003d SparkSession.builder.appName(\"KMeans notebook\").getOrCreate()\nsc \u003d sparkSession.sparkContext"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " \n\n## TODO 1\n\n **[CODE CHANGE REQUIRED]**\n\nComplete the definition of the following function `load_points_into_rdd`, which load the tsv data into an RDD given the namenode (e.g. `127.0.0.1:9000`) and the HDFS path (e.g. `kmeans/input`) \n\n\u003cstyle\u003e\n    div.hidecode + pre {display: none}\n\u003c/style\u003e\n\u003cscript\u003e\ndoclick\u003dfunction(e) {\n    e.nextSibling.nextSibling.style.display\u003d\"block\";\n}\n\u003c/script\u003e\n\n\u003cdiv class\u003d\"hidecode\" onclick\u003d\"doclick(this);\"\u003e[Show Hint]\u003c/div\u003e\n\n```python\nnamenode \u003d \"127.0.0.1:9000\"\n\ndef load_points_into_rdd(namenode, path): # TODO\n    return sc.textFile(\"hdfs://%s/%s\" % (namenode,path)).map(lambda ln:ln.strip().split(\"\\t\")).map(lambda l:(float(l[0]), float(l[1])))\n```"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark \n\nnamenode \u003d \"127.0.0.1:9000\"\n\ndef load_points_into_rdd(namenode, path): # TODO\n    return None\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Test case 1\n\nRun the following you should see, (the actual numeric values might differ, but the structure should be the same) \n```python\n[(61.4, -33.87), (19.57, -20.85), (22.95, -49.32), (42.81, 29.71), (-65.89, -75.57), (13.48, 71.92), (-17.28, -21.7), (1.79, 43.8), (11.58, -32.18), (1.73, -54.43)]\n```\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n\n# test case 1\npoints \u003d load_points_into_rdd(namenode, \"/lab13/ex3/input\")\npoints.take(10)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " \n\n## TODO 2\n **[CODE CHANGE REQUIRED]**\n \nComplete the following function `euc_dist` which compute the euclidean distance between two points\n\n\n$$\neucdist((x_1,y_1),(x_2,y_2)) \u003d \\sqrt{ (x_1 - x_2)^2 + (y_1 - y_2)^2  }\n$$\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\nimport math\ndef euc_dist(p1,p2): \n    \u0027\u0027\u0027\n    inpput\n    p1, p2: points\n    output\n    euclidean distance between p1 and p2\n    \u0027\u0027\u0027\n    # TODO:fixme\n    return 0\n    "
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Test case 2\n\nRun the following you should see, \n\n```text\n1.4142135623730951\n```\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n\np1 \u003d (3.0, 1.0)\np2 \u003d (2.0, 2.0)\n\neuc_dist(p1,p2)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " \n\n## TODO 3 \n **[CODE CHANGE REQUIRED]**\n \nComplete the following function which computes the mean of of an iterator/list of points.\n\n`mean()` functions takes a plain python iterator of points, not an RDD of points.\nThe mean of two points are defined as \n\n$$\nmean((x_1,y_1),(x_2,y_2)) \u003d (( x_1 + x_2) / 2, (y_1 + y_2) / 2)\n$$\n\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n\ndef mean(points):\n    \u0027\u0027\u0027\n    input\n    an iterator of points\n    output\n    a point that is the means of all the points in the input. if the input list is empty, return None\n    \u0027\u0027\u0027\n    # TODO: fixme\n    return (0,0)\n    "
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Test case 3\n\nRun the following you should see,\n\n```text\n(4,5)\n```\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\npoints \u003d ((x,y) for x in range(0, 10) for y in range(1,11))\n\nmean(points)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## TODO 4\n **[CODE CHANGE REQUIRED]**\nComplete the following function which finds the nearest centroids for each points in the RDD.\n\u003cstyle\u003e\n    div.hidecode + {display: none}\n\u003c/style\u003e\n\u003cscript\u003e\ndoclick\u003dfunction(e) {\n    e.nextSibling.nextSibling.style.display\u003d\"block\";\n}\n\u003c/script\u003e\n\n\u003cdiv class\u003d\"hidecode\" onclick\u003d\"doclick(this);\"\u003e[Show Hint]\u003c/div\u003e\n\n```text\nLet r1 and r2 be RDDs, r1.cartesion(r2) produces the cartesian product of r1 and r2. \nWhen finding the nearest centroid w.r.t to a point from a list of centroids, it is useful to think of it as a reduce operation.\n```\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n\ndef nearest(points, centroids):\n    \u0027\u0027\u0027\n    inputs\n    points: an RDD of points\n    centroids: an RDD of the current centroids\n    \n    output:\n    point_and_nearestcentroids : an RDD of pairs, each pair consists of a point and the nearest centroid it belongs to\n    \u0027\u0027\u0027\n    # TODO fixme\n    return None\n   "
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Test Case 4\n\nRun the following you should see, (the actual numeric values might differ, but the structure should be the same)\n\n\n```text\n\n[((31.34, 39.54), (10.94, 64.76)), ((63.18, -54.98), (78.66, -91.58)), ((-96.53, 54.69), (-53.93, 76.38)), ((-89.91, 9.5), (-42.27, 24.16)), ((95.43, 57.26), (10.94, 64.76)), ((74.38, -61.52), (78.66, -91.58)), ((77.21, -69.08), (78.66, -91.58)), ((-84.04, -74.41), (11.81, -27.94)), ((7.44, 45.33), (10.94, 64.76)), ((30.63, 52.08), (10.94, 64.76))]\n```"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n\npoints \u003d load_points_into_rdd(namenode, \"/lab13/ex3/input\")\ncentroids \u003d load_points_into_rdd(namenode, \"/lab13/ex3/centroids\")\n\nnearest(points, centroids).take(10)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## TODO 5\n\n **[CODE CHANGE REQUIRED]**\n\nComplete the following function which executes one iteration of the K-Means algorithm.\n\n\u003cstyle\u003e\n    div.hidecode + pre {display: none}\n\u003c/style\u003e\n\u003cscript\u003e\ndoclick\u003dfunction(e) {\n    e.nextSibling.nextSibling.style.display\u003d\"block\";\n}\n\u003c/script\u003e\n\n\u003cdiv class\u003d\"hidecode\" onclick\u003d\"doclick(this);\"\u003e[Show Hint]\u003c/div\u003e\n\n```text\nRecall the difference between reduceByKey and groupByKey\n```"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n\n    \ndef iteration(points, centroids):\n    \u0027\u0027\u0027\n    inputs\n    points: an RDD of points\n    centroids: an RDD of the current centroids\n    \n    output\n    current_and_new_centroids: an RDD of pairs, each pair consists of a current centroid and the new centroid\n    \u0027\u0027\u0027\n    # Step a: for each point, compute the euclidean_dinstance with each centroid, find the nearest centroid\n    point_and_nearcentroids \u003d nearest(points, centroids)\n    # Step b: flip the pairs from step b, and create an RDD of nearest centroid and points (note: the nearest centroids are still the current centroids)\n    # TODO fixme\n    nearcentroid_and_points \u003d None\n  \n    # Step c: compute the new centroids\n    # TODO fixme\n    current_and_newcentroids \u003d None\n   \n    return current_and_newcentroids\n    \n\n    "
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Test case 5\n\nRun the following you should see, (the actual numeric values might differ, but the structure should be the same)\n\n```text\n[((-42.27, 24.16), (-63.50617346938779, -4.715561224489797)), ((34.19, -5.47), (65.24037313432835, 4.253208955223882)), ((73.78, -94.16), (43.22766666666668, -84.53166666666665)), ((78.66, -91.58), (83.09037735849053, -62.727924528301884)), ((11.81, -27.94), (-18.85223809523809, -62.081285714285706)), ((-55.65, 97.37), (-42.820499999999996, 94.566)), ((11.1, 2.86), (8.047450980392156, 10.851764705882351)), ((10.94, 64.76), (38.894331550802164, 69.78598930481279)), ((-68.4, 93.57), (-83.12192307692308, 83.44)), ((-53.93, 76.38), (-56.9511111111111, 63.98825396825399))]\n```"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n\npoints \u003d load_points_into_rdd(namenode, \"/lab13/ex3/input\")\ncentroids \u003d load_points_into_rdd(namenode, \"/lab13/ex3/centroids\")\nr \u003d iteration(points,centroids)\nr.take(10)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " \n\n## KMeans \n\nTo define KMeans, we just need two more actions, \n\n1. `forAll(rdd,p)` which checks whether all elements in `rdd` satisfy the predicate `p`.\n2. `no_change(no_change(centroid_and_newcentroids,tolerance)` which applies a conditional check to all pairs of current and new centroids. It yields True if none of the new centroids is `None`, and the euclidean distances between the currents and new centroids are less than the tolerance.\n\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### TODO 6\n\nComplete the `forAll` function. You don\u0027t need to change `no_change` function. \n\n **[CODE CHANGE REQUIRED]**\n\nComplete the following function which executes one iteration of the K-Means algorithm.\n\n\u003cstyle\u003e\n    div.hidecode + pre {display: none}\n\u003c/style\u003e\n\u003cscript\u003e\ndoclick\u003dfunction(e) {\n    e.nextSibling.nextSibling.style.display\u003d\"block\";\n}\n\u003c/script\u003e\n\n\u003cdiv class\u003d\"hidecode\" onclick\u003d\"doclick(this);\"\u003e[Show Hint]\u003c/div\u003e\n\n```text\nYou can implement it using \n\n1. the aggregate function or\n2. map and reduce functions.\n\n```\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n\ndef forAll(rdd, p):\n    \u0027\u0027\u0027\n    input:\n    rdd : an RDD\n    p : a predicate, a lambda function that takes a value and return a boolean. p will be applied to all elements in rdd\n    \n    output:\n    True or False\n    \u0027\u0027\u0027\n    # TODO: fixme\n    return False\n   \n\n\ndef no_change(centroid_and_newcentroids,tolerance):\n    return forAll(centroid_and_newcentroids, lambda p:  p[1] is not None and euc_dist(p[0], p[1]) \u003c tolerance)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Test Case 6\n\nRun the following, we should see `True`\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n\ntest_vs \u003d sc.parallelize([2,4,0,6])\nforAll(test_vs ,lambda x:x % 2 \u003d\u003d 0)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Lastly, the `kmeans` function is defined by a for-loop in which \n\n1. we call `iteration` to compute the new centroids, \n2. then check whether there is any change between the current and new centroids via `no_change`. \n    2.1. If there is changes, it goes back to the loop by sending the new centroids to be current centroids, \n    2.2. otherwise it exits the loop and compute the membership between the points and the lastest centroids.\n\nNote the use of `.persist()`. try to re-run it again by commenting away the statements using `.persist()`, it will take a longer time to converge.\n\nThe code is written for you, you don\u0027t need to change anything unless you want to experiement with `.persist()` and without."
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n\n\ndef kmeans(points, centroids, num_iters, tolerance\u003d2):\n    points.persist()\n    for i in range(0, num_iters):\n        centroid_and_newcentroids \u003d iteration(points,centroids)\n        if no_change(centroid_and_newcentroids,tolerance):\n            break;\n        centroids \u003d centroid_and_newcentroids.map(lambda p:p[1]).filter(lambda c: c is not None)\n        centroids.persist()\n        # print(i,centroids.collect())\n    \n    return nearest(points, centroids)\n    \n    \n        "
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n\npoints \u003d load_points_into_rdd(namenode, \"/lab13/ex3/input\")\ncentroids \u003d load_points_into_rdd(namenode, \"/lab13/ex3/centroids\")\n\nclusters \u003d kmeans(points, centroids, 100, 2)\n\nclusters.take(100)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## modify and run the following to clean up the HDFS\n\n **[CODE CHANGE REQUIRED]**"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%sh\n/opt/hadoop-3.3.0/bin/hdfs dfs -rm -r /lab13/ex3/\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\nsc.stop()"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}